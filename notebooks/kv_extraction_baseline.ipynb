{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f00f5c4",
   "metadata": {},
   "source": [
    "# Imports + config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from collections import defaultdict\n",
    "import sys, pathlib\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f07c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path.cwd()\n",
    "print(f\"KV Extraction baseline project root: {project_root}\")\n",
    "sys.path.append(str(project_root))\n",
    "from src.kv_extraction import *\n",
    "from src.evaluation_utils import getenv_bool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020ec68-6c76-42ec-a51f-5abb460865f0",
   "metadata": {},
   "source": [
    "# Initial Setup and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9b820-dab5-4fb1-b854-b179522e902f",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The following should be set in your .env file\n",
    "BASE_URL - the URL of the API you are using to call the LLM\n",
    "MODEL_NAME - the name of the LLM model you are using\n",
    "API_KEY - your api key for LLM access\n",
    "\"\"\"\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "folder_name = \"SEC_WikiDB_subset\"\n",
    "data_type = \"unfiltered\"\n",
    "subset = \"all\"\n",
    "run_from_localdir = getenv_bool(\"RUN_LOCAL\", default=False)\n",
    "\n",
    "LITELLM_MODEL = os.environ[\"LITELLM_MODEL\"]\n",
    "LLM_API_BASE = os.environ[\"BASE_URL\"]\n",
    "model_name = os.environ[\"MODEL_NAME\"]\n",
    "rg_model_name = os.environ[\"RG_MODEL_NAME\"]\n",
    "API_KEY = os.environ[\"API_KEY\"]\n",
    "\n",
    "out_dir = os.environ[\"OUTPUT_DIR\"]\n",
    "max_workers = int(os.environ[\"MAX_WORKERS_KV\"])\n",
    "liteLLM_retries = int(os.environ[\"LITELLM_RETRIES\"])  # 15\n",
    "\n",
    "# Row sampling configuration\n",
    "# Number of rows to sample per dataset (None for all rows)\n",
    "ROW_SAMPLE_SIZE = os.environ[\"ROW_SAMPLE_SIZE\"]\n",
    "ROW_RANDOM_SEED = int(os.environ[\"ROW_RANDOM_SEED\"])\n",
    "randomize = False\n",
    "\n",
    "\n",
    "# Convert ROW_SAMPLE_SIZE to None if it's 'None'\n",
    "if ROW_SAMPLE_SIZE == \"None\":\n",
    "    ROW_SAMPLE_SIZE = None\n",
    "else:\n",
    "    ROW_SAMPLE_SIZE = int(ROW_SAMPLE_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de422a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_name = model_name\n",
    "out_name = out_name.replace(\".\", \"_\")\n",
    "out_name = Path(out_name).stem\n",
    "\n",
    "rg_model_name = rg_model_name.replace(\".\", \"_\")\n",
    "rg_model_name = Path(rg_model_name).stem\n",
    "\n",
    "max_threads = max_workers\n",
    "error_log = f\"{out_dir}/eval_reports_{folder_name}/kv_extraction_errors_{out_name}.txt\"\n",
    "Path(error_log).parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "wikidb_path = f\"{out_dir}/{folder_name}\"\n",
    "output_path = f\"{out_dir}/{folder_name}_{data_type}_{subset}\"\n",
    "\n",
    "# Path setup\n",
    "eval_reports_path = Path(f\"{out_dir}/eval_reports_{folder_name}\")\n",
    "eval_reports_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure lock directory exists\n",
    "output_lock_dir = f\"{out_dir}/locks\"\n",
    "os.makedirs(output_lock_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c47cd9-d692-4ab8-9820-e95331c12165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load subset for faster experimentation. \"SEC_WikiDB subset unfiltered - all file types\" - The smaller 49 csv files for quick prototyping.\n",
    "dataset = load_dataset(\n",
    "    \"ibm-research/struct-text\",\n",
    "    f\"{folder_name}_{data_type}_{subset}\",\n",
    "    streaming=False,\n",
    "    cache_dir=output_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c107e6",
   "metadata": {},
   "source": [
    "# Model Configs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model: {model_name}, Output name: {out_name}, RG model: {rg_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_args = {\n",
    "    \"model\": f\"{LITELLM_MODEL}/{model_name}\",\n",
    "    \"api_base\": f\"{LLM_API_BASE}\",\n",
    "    \"temperature\": 0,\n",
    "    \"api_key\": API_KEY,\n",
    "    \"num_retries\": liteLLM_retries,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd05ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_llm_response(\"Ping\", API_KEY, completion_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de873f3c-bf6f-4c33-9701-a39dae902570",
   "metadata": {},
   "source": [
    "# Example run to process a target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05895588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb7dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\n",
    "    \"ground_truth\",\n",
    "    \"report_types\",\n",
    "    \"generated_reports\",\n",
    "]  # Hardcoded: See HF ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_from_localdir:\n",
    "    splits = {\n",
    "        \"train\": dataset[\"train\"],\n",
    "        \"val\": dataset[\"validation\"],\n",
    "        \"test\": dataset[\"test\"],\n",
    "    }\n",
    "    # this is the only run available on hf.\n",
    "    rg_model_name = \"Qwen2_5-72B-Instruct\"\n",
    "\n",
    "    for split_type, split_data in splits.items():\n",
    "        # create a dictionary to group files by dataset_name:\n",
    "        dataset_groups = defaultdict(\n",
    "            lambda: {\n",
    "                \"ground_truth\": None,\n",
    "                \"report_types\": None,\n",
    "                \"generated_reports\": None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # First split by all files by datast name:\n",
    "        for idx, split_row in enumerate(split_data):\n",
    "            row_label = label_names[split_row[\"report_type\"]]\n",
    "            file_name = split_row[\"file_name\"]\n",
    "\n",
    "            # extract the dataset name based on the file type:\n",
    "            if \"_ground_truth.csv\" in file_name:\n",
    "                dataset_name = file_name.replace(\"_ground_truth.csv\", \"\")\n",
    "                dataset_groups[dataset_name][\"ground_truth\"] = (idx, split_row)\n",
    "\n",
    "            if \"_report_types_\" in file_name:\n",
    "                dataset_name = file_name.replace(\n",
    "                    f\"_report_types_{rg_model_name}.csv\", \"\"\n",
    "                )\n",
    "                dataset_groups[dataset_name][\"report_types\"] = (idx, split_row)\n",
    "\n",
    "            if \"_generated_reports_\" in file_name:\n",
    "                dataset_name = file_name.replace(\n",
    "                    f\"_generated_reports_{rg_model_name}.csv\", \"\"\n",
    "                )\n",
    "                dataset_groups[dataset_name][\"generated_reports\"] = (idx, split_row)\n",
    "\n",
    "        ######################################################################################\n",
    "        # now process with each dataset group with all three files:\n",
    "        for dataset_name, files in dataset_groups.items():\n",
    "            if all(files.values()):\n",
    "                print(f\"\\nProcessing dataset: {dataset_name} in {split_type}\")\n",
    "                # Extract the data for all three files\n",
    "                ground_truth_data = files[\"ground_truth\"][1]\n",
    "                report_types_data = files[\"report_types\"][1]\n",
    "                generated_reports_data = files[\"generated_reports\"][1]\n",
    "\n",
    "                print(ground_truth_data[\"file_name\"], ground_truth_data[\"report_type\"])\n",
    "                print(report_types_data[\"file_name\"], report_types_data[\"report_type\"])\n",
    "                print(\n",
    "                    generated_reports_data[\"file_name\"],\n",
    "                    generated_reports_data[\"report_type\"],\n",
    "                )\n",
    "\n",
    "                process_dataset(\n",
    "                    dataset_name=dataset_name,\n",
    "                    split_type=split_type,\n",
    "                    ground_truth_data=ground_truth_data,\n",
    "                    report_types_data=report_types_data,\n",
    "                    generated_reports_data=generated_reports_data,\n",
    "                    run_from_localdir=run_from_localdir,\n",
    "                    output_path=output_path,\n",
    "                    folder_name=folder_name,\n",
    "                    out_name=out_name,\n",
    "                    eval_reports_path=eval_reports_path,\n",
    "                    rg_model_name=rg_model_name,\n",
    "                    error_log=error_log,\n",
    "                    API_KEY=API_KEY,\n",
    "                    completion_args=completion_args,\n",
    "                    ROW_SAMPLE_SIZE=ROW_SAMPLE_SIZE,\n",
    "                    ROW_RANDOM_SEED=ROW_RANDOM_SEED,\n",
    "                    max_workers=max_workers,\n",
    "                )\n",
    "\n",
    "\n",
    "else:\n",
    "    for split_type in [\"train\", \"val\", \"test\"]:\n",
    "        # for split_type in [\"val\", \"test\"]:\n",
    "        split_dir = os.path.join(output_path, split_type)\n",
    "        if not os.path.exists(split_dir):\n",
    "            print(f\"Split directory {split_dir} does not exist. Skipping {split_type}.\")\n",
    "            continue\n",
    "\n",
    "        # create a dictionary to group files by dataset_name:\n",
    "        dataset_groups = defaultdict(\n",
    "            lambda: {\n",
    "                \"ground_truth\": None,\n",
    "                \"report_types\": None,\n",
    "                \"generated_reports\": None,\n",
    "            }\n",
    "        )\n",
    "        # List all CSV files in the split directory\n",
    "        for file_name in os.listdir(split_dir):\n",
    "            if not file_name.endswith(\".csv\"):\n",
    "                continue\n",
    "\n",
    "            full_path = os.path.join(split_dir, file_name)\n",
    "            with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                csv_text = f.read()\n",
    "\n",
    "            # extract the dataset name based on the file type:\n",
    "            if \"_ground_truth.csv\" in file_name:\n",
    "                dataset_name = file_name.replace(\"_ground_truth.csv\", \"\")\n",
    "                dataset_groups[dataset_name][\"ground_truth\"] = {\n",
    "                    \"file_name\": file_name,\n",
    "                    \"report_type\": 0,  # Dummy value\n",
    "                }\n",
    "\n",
    "            if \"_report_types_\" in file_name:\n",
    "                dataset_name = file_name.replace(\n",
    "                    f\"_report_types_{rg_model_name}.csv\", \"\"\n",
    "                )\n",
    "                dataset_groups[dataset_name][\"report_types\"] = {\n",
    "                    \"file_name\": file_name,\n",
    "                    \"report_type\": 1,  # Dummy value\n",
    "                }\n",
    "\n",
    "            if \"_generated_reports_\" in file_name:\n",
    "                dataset_name = file_name.replace(\n",
    "                    f\"_generated_reports_{rg_model_name}.csv\", \"\"\n",
    "                )\n",
    "                dataset_groups[dataset_name][\"generated_reports\"] = {\n",
    "                    \"file_name\": file_name,\n",
    "                    \"report_type\": 2,  # Dummy value\n",
    "                }\n",
    "        ######################################################################################\n",
    "        # now process each dataset group with all three files:\n",
    "        for dataset_name, files in dataset_groups.items():\n",
    "            # local mode: Assume data is structured in output_path/split_type/ with CSV files\n",
    "            print(\n",
    "                f\"RUNNING LOCALLY GENERATED REPORTS: LABEL: run_from_localdir {run_from_localdir}\"\n",
    "            )\n",
    "            if all(files.values()):\n",
    "                print(f\"\\nProcessing dataset: {dataset_name} in {split_type}\")\n",
    "                # Extract the data for all three files\n",
    "                ground_truth_data = files[\"ground_truth\"]\n",
    "                report_types_data = files[\"report_types\"]\n",
    "                generated_reports_data = files[\"generated_reports\"]\n",
    "\n",
    "                print(ground_truth_data[\"file_name\"], ground_truth_data[\"report_type\"])\n",
    "                print(report_types_data[\"file_name\"], report_types_data[\"report_type\"])\n",
    "                print(\n",
    "                    generated_reports_data[\"file_name\"],\n",
    "                    generated_reports_data[\"report_type\"],\n",
    "                )\n",
    "                process_dataset(\n",
    "                    dataset_name=dataset_name,\n",
    "                    split_type=split_type,\n",
    "                    ground_truth_data=ground_truth_data,\n",
    "                    report_types_data=report_types_data,\n",
    "                    generated_reports_data=generated_reports_data,\n",
    "                    run_from_localdir=run_from_localdir,\n",
    "                    output_path=output_path,\n",
    "                    folder_name=folder_name,\n",
    "                    out_name=out_name,\n",
    "                    eval_reports_path=eval_reports_path,\n",
    "                    rg_model_name=rg_model_name,\n",
    "                    error_log=error_log,\n",
    "                    API_KEY=API_KEY,\n",
    "                    completion_args=completion_args,\n",
    "                    ROW_SAMPLE_SIZE=ROW_SAMPLE_SIZE,\n",
    "                    ROW_RANDOM_SEED=ROW_RANDOM_SEED,\n",
    "                    max_workers=max_workers,\n",
    "                )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tada_bench",
   "language": "python",
   "name": "tada_bench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
