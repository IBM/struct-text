{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "df8f9389",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import re\n",
                "import json\n",
                "import sys\n",
                "import time\n",
                "import random\n",
                "import filelock\n",
                "import io\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "from typing import Dict, Any, List\n",
                "from tqdm.notebook import tqdm\n",
                "import threading\n",
                "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
                "from collections import defaultdict\n",
                "\n",
                "# LangChain and DSPy\n",
                "from langchain.prompts import ChatPromptTemplate\n",
                "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
                "import dspy\n",
                "\n",
                "# Environment setup\n",
                "from dotenv import load_dotenv\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b90d4cf6",
            "metadata": {},
            "outputs": [],
            "source": [
                "project_root = Path.cwd().parent\n",
                "sys.path.append(str(project_root))\n",
                "\n",
                "from src.evaluation_utils import TextQualityEvaluator, getenv_bool\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8457effd",
            "metadata": {},
            "source": [
                "# Initial Setup and configs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "89465c31",
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "load_dotenv()\n",
                "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
                "\n",
                "\n",
                "force_reprocess = True\n",
                "\n",
                "\n",
                "folder_name = \"SEC_WikiDB_subset\"\n",
                "data_type = \"unfiltered\"\n",
                "subset = \"all\"\n",
                "run_from_localdir = getenv_bool(\"RUN_LOCAL\", default=False)\n",
                "\n",
                "\n",
                "LITELLM_MODEL = os.environ[\"LITELLM_MODEL\"]\n",
                "LLM_API_BASE = os.environ[\"BASE_URL\"]\n",
                "model_name = os.environ[\"MODEL_NAME\"]\n",
                "rg_model_name = os.environ[\"RG_MODEL_NAME\"]\n",
                "API_KEY = os.environ[\"API_KEY\"]\n",
                "\n",
                "out_dir = os.environ[\"OUTPUT_DIR\"]\n",
                "max_workers = int(os.environ[\"MAX_WORKERS\"])\n",
                "liteLLM_retries = int(os.environ[\"LITELLM_RETRIES\"])  # 15\n",
                "\n",
                "# Row sampling configuration\n",
                "# Number of rows to sample per dataset (None for all rows)\n",
                "ROW_SAMPLE_SIZE = os.environ[\"ROW_SAMPLE_SIZE\"]\n",
                "ROW_RANDOM_SEED = int(os.environ[\"ROW_RANDOM_SEED\"])\n",
                "\n",
                "# Convert ROW_SAMPLE_SIZE to None if it's 'None'\n",
                "if ROW_SAMPLE_SIZE == \"None\":\n",
                "    ROW_SAMPLE_SIZE = None\n",
                "else:\n",
                "    ROW_SAMPLE_SIZE = int(ROW_SAMPLE_SIZE)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f2820502",
            "metadata": {},
            "outputs": [],
            "source": [
                "out_name = model_name\n",
                "out_name = out_name.replace(\".\", \"_\")\n",
                "out_name = Path(out_name).stem\n",
                "\n",
                "\n",
                "rg_model_name = rg_model_name.replace(\".\", \"_\")\n",
                "rg_model_name = Path(rg_model_name).stem\n",
                "\n",
                "# Path setup\n",
                "output_path = f\"{out_dir}/{folder_name}_{data_type}_{subset}\"\n",
                "eval_reports_path = Path(f\"{out_dir}/eval_reports_{folder_name}\")\n",
                "eval_reports_path.mkdir(parents=True, exist_ok=True)\n",
                "error_log = f\"{eval_reports_path}/llm_judge_errors_{out_name}.txt\"\n",
                "\n",
                "\n",
                "output_lock_dir = f\"{out_dir}/locks\"  # Directory for file locks\n",
                "os.makedirs(output_lock_dir, exist_ok=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cce8b778",
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "# Load subset for faster experimentation. \"SEC_WikiDB subset unfiltered - all file types\" - The smaller 49 csv files for quick prototyping.\n",
                "dataset = load_dataset(\n",
                "    \"ibm-research/struct-text\",\n",
                "    f\"{folder_name}_{data_type}_{subset}\",\n",
                "    streaming=False,\n",
                "    cache_dir=output_path,\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a1cbef72",
            "metadata": {},
            "source": [
                "# Model Configs "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "69190b4a",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Model: {model_name}, Output name: {out_name}, RG model: {rg_model_name}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "58ae53e8",
            "metadata": {},
            "outputs": [],
            "source": [
                "completion_args = {\n",
                "    \"model\": f\"{LITELLM_MODEL}/{model_name}\",\n",
                "    \"api_base\": f\"{LLM_API_BASE}\",\n",
                "    \"temperature\": 0,\n",
                "    \"api_key\": API_KEY,\n",
                "    \"cache\": 0,\n",
                "    \"num_retries\": liteLLM_retries,\n",
                "}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3c70c3ea",
            "metadata": {},
            "outputs": [],
            "source": [
                "lm = dspy.LM(**completion_args)\n",
                "dspy.configure(lm=lm)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c5b3ec73",
            "metadata": {},
            "outputs": [],
            "source": [
                "resp = lm(\"Ping\")\n",
                "print(resp)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d5d8644e",
            "metadata": {},
            "outputs": [],
            "source": [
                "text_quality_evaluator = TextQualityEvaluator()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "42cb57b6",
            "metadata": {},
            "source": [
                "# Parallel Processing "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7143462c",
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_dataset(\n",
                "    dataset_name,\n",
                "    ground_truth_data,\n",
                "    report_types_data,\n",
                "    generated_reports_data,\n",
                "    split_type,\n",
                "    rg_model_name,\n",
                "):\n",
                "    \"\"\"Process a single dataset for LLM judge evaluation.\"\"\"\n",
                "    # Read metadata (with lock to avoid race conditions)\n",
                "    meta_csv_path = (\n",
                "        Path(output_path).parent / f\"meta_data_{rg_model_name}_{folder_name}.csv\"\n",
                "    )\n",
                "    with filelock.FileLock(f\"{output_lock_dir}/metadata.lock\"):\n",
                "        if run_from_localdir:\n",
                "            # Read metadata\n",
                "            if not meta_csv_path.exists():\n",
                "                raise FileNotFoundError(f\"Metadata file not found at {meta_csv_path}\")\n",
                "            meta_df = pd.read_csv(meta_csv_path)\n",
                "        else:\n",
                "            meta_ds = load_dataset(\n",
                "                \"ibm-research/struct-text\",\n",
                "                data_files=f\"meta_data_{rg_model_name}_{folder_name}.csv\",\n",
                "            )\n",
                "            meta_df = meta_ds[\"train\"].to_pandas()\n",
                "            # save first:\n",
                "            meta_df.to_csv(meta_csv_path, index=False)\n",
                "\n",
                "    # Get the row for this dataset\n",
                "    dataset_row = meta_df[meta_df[\"dataset_name\"] == dataset_name]\n",
                "    if len(dataset_row) == 0:\n",
                "        raise ValueError(f\"Dataset '{dataset_name}' not found in metadata.\")\n",
                "    if len(dataset_row) > 1:\n",
                "        raise ValueError(f\"Multiple entries for dataset '{dataset_name}' in metadata.\")\n",
                "\n",
                "    # Get the single row\n",
                "    row = dataset_row.iloc[0]\n",
                "\n",
                "    # Define output path and column\n",
                "    column_name = f\"text_quality_csv_path_{out_name}\"\n",
                "    output_csv = Path(eval_reports_path) / split_type\n",
                "    Path(output_csv).mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "    output_csv = (\n",
                "        output_csv / f\"{dataset_name}_{rg_model_name}_text_quality_{out_name}.csv\"\n",
                "    )\n",
                "    # Skip if already processed\n",
                "    existing_path = row.get(column_name, pd.NA)\n",
                "    if (\n",
                "        not force_reprocess\n",
                "        and not pd.isna(existing_path)\n",
                "        and os.path.exists(existing_path)\n",
                "        and os.path.getsize(existing_path) > 0\n",
                "    ):\n",
                "        print(f\"Skipping {dataset_name} - already processed\")\n",
                "        return\n",
                "\n",
                "    # Read data\n",
                "\n",
                "    if run_from_localdir:\n",
                "        original_csv = row[\"ground_truth_csv_path\"]\n",
                "        planned_csv = row[\"report_types_csv_path\"]\n",
                "        generated_csv = row[\"generated_reports_csv_path\"]\n",
                "\n",
                "        print(f\"Processing {dataset_name}\")\n",
                "        print(f\"Loading data from: {original_csv}, {planned_csv}, {generated_csv}\")\n",
                "\n",
                "        orig_df = pd.read_csv(original_csv)\n",
                "        planned_df = pd.read_csv(planned_csv)\n",
                "        generated_df = pd.read_csv(generated_csv)\n",
                "        row_plan = planned_df.iloc[0]\n",
                "    else:\n",
                "        # process from the hf datasets. The dataset_row should have that info:\n",
                "        orig_df = pd.read_csv(io.StringIO(ground_truth_data[\"csv_text\"]))\n",
                "        planned_df = pd.read_csv(io.StringIO(report_types_data[\"csv_text\"]))\n",
                "        generated_df = pd.read_csv(io.StringIO(generated_reports_data[\"csv_text\"]))\n",
                "        row_plan = planned_df.iloc[0]\n",
                "\n",
                "    # Sample rows if requested:\n",
                "    total_rows = len(generated_df)\n",
                "    if ROW_SAMPLE_SIZE is not None and total_rows > ROW_SAMPLE_SIZE:\n",
                "        # use fixed seed for reproducible sampling unique to the dataset as well.\n",
                "        # apparently random uses a hashmap and can take in unique strings as well\n",
                "        random.seed(f\"{ROW_RANDOM_SEED}_{dataset_name}\")\n",
                "        sampled_indices = random.sample(range(total_rows), ROW_SAMPLE_SIZE)\n",
                "        sampled_indices.sort()\n",
                "\n",
                "        orig_df_sampled = orig_df.iloc[sampled_indices].reset_index(drop=True)\n",
                "        generated_df_sampled = generated_df.iloc[sampled_indices].reset_index(drop=True)\n",
                "        print(f\"Sampled {len(sampled_indices)} rows from {total_rows} total rows\")\n",
                "    else:\n",
                "        # Use all the rows\n",
                "        orig_df_sampled = orig_df\n",
                "        generated_df_sampled = generated_df\n",
                "        sampled_indices = list(range(total_rows))\n",
                "        print(f\"Processing all {total_rows} rows (no sampling)\")\n",
                "\n",
                "    # Initialize results list\n",
                "    text_quality_data = [None] * len(sampled_indices)\n",
                "\n",
                "    #############################################################\n",
                "    def process_llm_as_judge(sample_idx, original_idx):\n",
                "        try:\n",
                "            row_orig = orig_df_sampled.iloc[sample_idx]\n",
                "            row_gen = generated_df_sampled.iloc[sample_idx]\n",
                "            result = text_quality_evaluator(\n",
                "                source_data=row_orig.to_dict(), generated_report=row_gen.to_dict()\n",
                "            )\n",
                "            data_out = {\n",
                "                \"original_index\": original_idx,  # Keep track of original index\n",
                "                \"src_data\": result.source_data,\n",
                "                \"gen_text\": result.generated_report,\n",
                "                \"factual_claims\": result.claim_analysis,\n",
                "                \"factuality_score\": result.factuality_score,\n",
                "                \"factual_reasoning\": result.factuality_reasoning,\n",
                "                \"hallucination_claims\": result.statement_analysis,\n",
                "                \"hallucination_score\": result.hallucination_score,\n",
                "                \"hallucination_reasoning\": result.hallucination_reasoning,\n",
                "                \"coherence_claims\": result.coherence_issues,\n",
                "                \"coherence_score\": result.coherence_score,\n",
                "                \"coherence_reasoning\": result.coherence_reasoning,\n",
                "                \"overall_quality_score\": result.overall_quality_score,\n",
                "            }\n",
                "            return sample_idx, data_out, None\n",
                "        except Exception as e:\n",
                "            error_msg = f\"Error processing {dataset_name}, row {original_idx}: {str(e)}\"\n",
                "            return (\n",
                "                sample_idx,\n",
                "                {},\n",
                "                error_msg,\n",
                "            )\n",
                "\n",
                "    # #############################################################\n",
                "    # Process rows in parallel\n",
                "    result_lock = threading.Lock()\n",
                "\n",
                "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
                "        future_to_idx = {}\n",
                "        for i, original_idx in enumerate(sampled_indices):\n",
                "            future = executor.submit(process_llm_as_judge, i, original_idx)\n",
                "            future_to_idx[future] = i\n",
                "\n",
                "        with tqdm(\n",
                "            total=len(sampled_indices), desc=f\"LLM judge eval for {dataset_name}\"\n",
                "        ) as pbar:\n",
                "            for future in as_completed(future_to_idx):\n",
                "                idx = future_to_idx[future]\n",
                "                try:\n",
                "                    sample_idx, data_out, error = future.result()\n",
                "                    if data_out is not None:\n",
                "                        with result_lock:\n",
                "                            text_quality_data[sample_idx] = data_out\n",
                "                    if error is not None:\n",
                "                        with open(error_log, \"a\") as f:\n",
                "                            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
                "                            f.write(\n",
                "                                f\"{timestamp} - {dataset_name} - {idx} (original: {data_out['original_index']}) - {error}\\n\"\n",
                "                            )\n",
                "                except Exception as e:\n",
                "                    print(f\"Error handling result for {idx}: {str(e)}\")\n",
                "                pbar.update(1)\n",
                "    ############################################\n",
                "    # Clean None values and create DataFrame\n",
                "    text_quality_data = [\n",
                "        (\n",
                "            item\n",
                "            if isinstance(item, dict)\n",
                "            else {\"error\": \"Missing result\", \"original_index\": sampled_indices[i]}\n",
                "        )\n",
                "        for i, item in enumerate(text_quality_data)\n",
                "    ]\n",
                "    text_quality_df = pd.DataFrame(text_quality_data)\n",
                "\n",
                "    # Add sampling metadata\n",
                "    sampling_info = {\n",
                "        \"dataset_name\": dataset_name,\n",
                "        \"total_rows\": total_rows,\n",
                "        \"sampled_rows\": len(sampled_indices),\n",
                "        \"sampling_method\": (\n",
                "            \"random\" if ROW_SAMPLE_SIZE and total_rows > ROW_SAMPLE_SIZE else \"full\"\n",
                "        ),\n",
                "        \"sample_seed\": (\n",
                "            f\"{ROW_RANDOM_SEED}_{dataset_name}\"\n",
                "            if ROW_SAMPLE_SIZE and total_rows > ROW_SAMPLE_SIZE\n",
                "            else None\n",
                "        ),\n",
                "        \"processing_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
                "    }\n",
                "\n",
                "    # Save sampling info and results\n",
                "    with open(output_csv.with_suffix(\".json\"), \"w\") as f:\n",
                "        json.dump(sampling_info, f, indent=2)\n",
                "\n",
                "    # Write results\n",
                "    text_quality_df.to_csv(output_csv, index=False)\n",
                "    print(f\"Results saved to {output_csv}\")\n",
                "\n",
                "    # Update metadata with a file lock to prevent race conditions\n",
                "    with filelock.FileLock(f\"{output_lock_dir}/metadata.lock\"):\n",
                "        # Re-read metadata to get the latest version\n",
                "        meta_df = pd.read_csv(meta_csv_path)\n",
                "        # Ensure the dataset still exists\n",
                "        if dataset_name not in meta_df[\"dataset_name\"].values:\n",
                "            print(\n",
                "                f\"Warning: Dataset {dataset_name} no longer in metadata. Skipping update.\"\n",
                "            )\n",
                "            return\n",
                "\n",
                "        # Find the row again (index might have changed)\n",
                "        row_idx = meta_df[meta_df[\"dataset_name\"] == dataset_name].index[0]\n",
                "        # Update the metadata\n",
                "        meta_df.at[row_idx, column_name] = str(output_csv)\n",
                "\n",
                "        # Save the metadata\n",
                "        meta_df.to_csv(meta_csv_path, index=False)\n",
                "\n",
                "    print(f\"Metadata updated for {dataset_name}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cf040262",
            "metadata": {},
            "source": [
                "# Run"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a82c3136",
            "metadata": {},
            "outputs": [],
            "source": [
                "label_names = [\n",
                "    \"ground_truth\",\n",
                "    \"report_types\",\n",
                "    \"generated_reports\",\n",
                "]  # Hardcoded: See HF ;\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2f970467",
            "metadata": {},
            "outputs": [],
            "source": [
                "if not run_from_localdir:\n",
                "    splits = {\n",
                "        \"train\": dataset[\"train\"],\n",
                "        \"val\": dataset[\"validation\"],\n",
                "        \"test\": dataset[\"test\"],\n",
                "    }\n",
                "    # this is the only run available on hf.\n",
                "    rg_model_name = \"Qwen2_5-72B-Instruct\"\n",
                "\n",
                "    for split_type, split_data in splits.items():\n",
                "        # create a dictionary to group files by dataset_name:\n",
                "        dataset_groups = defaultdict(\n",
                "            lambda: {\n",
                "                \"ground_truth\": None,\n",
                "                \"report_types\": None,\n",
                "                \"generated_reports\": None,\n",
                "            }\n",
                "        )\n",
                "\n",
                "        # First split by all files by datast name:\n",
                "        for idx, split_row in enumerate(split_data):\n",
                "            row_label = label_names[split_row[\"report_type\"]]\n",
                "            file_name = split_row[\"file_name\"]\n",
                "\n",
                "            # extract the dataset name based on the file type:\n",
                "            if \"_ground_truth.csv\" in file_name:\n",
                "                dataset_name = file_name.replace(\"_ground_truth.csv\", \"\")\n",
                "                dataset_groups[dataset_name][\"ground_truth\"] = (idx, split_row)\n",
                "\n",
                "            if \"_report_types_\" in file_name:\n",
                "                dataset_name = file_name.replace(\n",
                "                    f\"_report_types_{rg_model_name}.csv\", \"\"\n",
                "                )\n",
                "                dataset_groups[dataset_name][\"report_types\"] = (idx, split_row)\n",
                "\n",
                "            if \"_generated_reports_\" in file_name:\n",
                "                dataset_name = file_name.replace(\n",
                "                    f\"_generated_reports_{rg_model_name}.csv\", \"\"\n",
                "                )\n",
                "                dataset_groups[dataset_name][\"generated_reports\"] = (idx, split_row)\n",
                "\n",
                "        ######################################################################################\n",
                "        # now process with each dataset group with all three files:\n",
                "        for dataset_name, files in dataset_groups.items():\n",
                "            if all(files.values()):\n",
                "                print(f\"\\nProcessing dataset: {dataset_name} in {split_type}\")\n",
                "                # Extract the data for all three files\n",
                "                ground_truth_data = files[\"ground_truth\"][1]\n",
                "                report_types_data = files[\"report_types\"][1]\n",
                "                generated_reports_data = files[\"generated_reports\"][1]\n",
                "\n",
                "                print(ground_truth_data[\"file_name\"], ground_truth_data[\"report_type\"])\n",
                "                print(report_types_data[\"file_name\"], report_types_data[\"report_type\"])\n",
                "                print(\n",
                "                    generated_reports_data[\"file_name\"],\n",
                "                    generated_reports_data[\"report_type\"],\n",
                "                )\n",
                "\n",
                "                process_dataset(\n",
                "                    dataset_name,\n",
                "                    ground_truth_data,\n",
                "                    report_types_data,\n",
                "                    generated_reports_data,\n",
                "                    split_type,\n",
                "                    rg_model_name,\n",
                "                )\n",
                "\n",
                "\n",
                "else:\n",
                "    for split_type in [\"train\", \"val\", \"test\"]:\n",
                "        # for split_type in [\"val\", \"test\"]:\n",
                "        split_dir = os.path.join(output_path, split_type)\n",
                "        if not os.path.exists(split_dir):\n",
                "            print(f\"Split directory {split_dir} does not exist. Skipping {split_type}.\")\n",
                "            continue\n",
                "\n",
                "        # create a dictionary to group files by dataset_name:\n",
                "        dataset_groups = defaultdict(\n",
                "            lambda: {\n",
                "                \"ground_truth\": None,\n",
                "                \"report_types\": None,\n",
                "                \"generated_reports\": None,\n",
                "            }\n",
                "        )\n",
                "        # List all CSV files in the split directory\n",
                "        for file_name in os.listdir(split_dir):\n",
                "            if not file_name.endswith(\".csv\"):\n",
                "                continue\n",
                "\n",
                "            full_path = os.path.join(split_dir, file_name)\n",
                "            with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
                "                csv_text = f.read()\n",
                "\n",
                "            # extract the dataset name based on the file type:\n",
                "            if \"_ground_truth.csv\" in file_name:\n",
                "                dataset_name = file_name.replace(\"_ground_truth.csv\", \"\")\n",
                "                dataset_groups[dataset_name][\"ground_truth\"] = {\n",
                "                    \"file_name\": file_name,\n",
                "                    \"report_type\": 0,  # Dummy value\n",
                "                }\n",
                "\n",
                "            if \"_report_types_\" in file_name:\n",
                "                dataset_name = file_name.replace(\n",
                "                    f\"_report_types_{rg_model_name}.csv\", \"\"\n",
                "                )\n",
                "                dataset_groups[dataset_name][\"report_types\"] = {\n",
                "                    \"file_name\": file_name,\n",
                "                    \"report_type\": 1,  # Dummy value\n",
                "                }\n",
                "\n",
                "            if \"_generated_reports_\" in file_name:\n",
                "                dataset_name = file_name.replace(\n",
                "                    f\"_generated_reports_{rg_model_name}.csv\", \"\"\n",
                "                )\n",
                "                dataset_groups[dataset_name][\"generated_reports\"] = {\n",
                "                    \"file_name\": file_name,\n",
                "                    \"report_type\": 2,  # Dummy value\n",
                "                }\n",
                "        ######################################################################################\n",
                "        # now process each dataset group with all three files:\n",
                "        for dataset_name, files in dataset_groups.items():\n",
                "            # local mode: Assume data is structured in output_path/split_type/ with CSV files\n",
                "            print(\n",
                "                f\"RUNNING LOCALLY GENERATED REPORTS: LABEL: run_from_localdir {run_from_localdir}\"\n",
                "            )\n",
                "            if all(files.values()):\n",
                "                print(f\"\\nProcessing dataset: {dataset_name} in {split_type}\")\n",
                "                # Extract the data for all three files\n",
                "                ground_truth_data = files[\"ground_truth\"]\n",
                "                report_types_data = files[\"report_types\"]\n",
                "                generated_reports_data = files[\"generated_reports\"]\n",
                "\n",
                "                print(ground_truth_data[\"file_name\"], ground_truth_data[\"report_type\"])\n",
                "                print(report_types_data[\"file_name\"], report_types_data[\"report_type\"])\n",
                "                print(\n",
                "                    generated_reports_data[\"file_name\"],\n",
                "                    generated_reports_data[\"report_type\"],\n",
                "                )\n",
                "\n",
                "                process_dataset(\n",
                "                    dataset_name,\n",
                "                    ground_truth_data,\n",
                "                    report_types_data,\n",
                "                    generated_reports_data,\n",
                "                    split_type,\n",
                "                    rg_model_name,\n",
                "                )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "afccdb36",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "tada_bench",
            "language": "python",
            "name": "tada_bench"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
