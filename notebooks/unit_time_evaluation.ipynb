{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6050e96",
   "metadata": {},
   "source": [
    "# Initial setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b5114f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Initial setup\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Literal\n",
    "from pydantic import BaseModel, Field, PydanticUndefinedAnnotation\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dateutil\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "import dspy\n",
    "\n",
    "# Execution and Multithreading\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging, logging.handlers, queue, threading\n",
    "from tqdm import tqdm\n",
    "import filelock\n",
    "import ast\n",
    "\n",
    "# Environment setup\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from contextlib import contextmanager, redirect_stdout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03cc7f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "project_root = Path.cwd().parent\n",
    "print(f\"Unit time accuracy project root: {project_root}\")\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.evaluation_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROW_LOG = logging.getLogger(\"row\")\n",
    "\n",
    "\n",
    "# hotfix this back to the\n",
    "@contextmanager\n",
    "def capture_row():\n",
    "    \"\"\"\n",
    "    Temporariliy diver all records from `logger_name` into an in-memory StringIO Buffer.\n",
    "    yeilds the buffer and resores the logger afterwards\n",
    "    \"\"\"\n",
    "    log_buf = io.StringIO()\n",
    "    out_buf = io.StringIO()\n",
    "\n",
    "    # logging.getLogger(logger_name)\n",
    "    hdlr = logging.StreamHandler(log_buf)\n",
    "    hdlr.setFormatter(logging.Formatter(\"%(message)s\"))\n",
    "    ROW_LOG.addHandler(hdlr)\n",
    "    ROW_LOG.propagate = False\n",
    "\n",
    "    with redirect_stdout(out_buf):\n",
    "        try:\n",
    "            yield lambda: log_buf.getvalue() + out_buf.getvalue()\n",
    "        finally:\n",
    "            ROW_LOG.removeHandler(hdlr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd0d697",
   "metadata": {},
   "source": [
    "# Initial setup and configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93f3b2",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "\n",
    "# Row sampling configuration\n",
    "output_stream_to_console = True\n",
    "is_verbose = False\n",
    "force_reprocess = True\n",
    "exec_parallel_run = True  # True\n",
    "# if you want the logging stream to also output onto the console:\n",
    "\n",
    "\n",
    "folder_name = \"SEC_WikiDB_subset\"\n",
    "data_type = \"unfiltered\"\n",
    "subset = \"all\"\n",
    "run_from_localdir = getenv_bool(\"RUN_LOCAL\", default=False)\n",
    "\n",
    "\n",
    "LITELLM_MODEL = os.environ[\"LITELLM_MODEL\"]\n",
    "LLM_API_BASE = os.environ[\"BASE_URL\"]\n",
    "model_name = os.environ[\"MODEL_NAME\"]\n",
    "rg_model_name = os.environ[\"RG_MODEL_NAME\"]\n",
    "API_KEY = os.environ[\"API_KEY\"]\n",
    "\n",
    "out_dir = os.environ[\"OUTPUT_DIR\"]\n",
    "max_workers = int(os.environ[\"MAX_WORKERS\"])\n",
    "\n",
    "\n",
    "# Number of rows to sample per dataset (None for all rows)\n",
    "ROW_SAMPLE_SIZE = os.environ[\"ROW_SAMPLE_SIZE\"]\n",
    "ROW_RANDOM_SEED = int(os.environ[\"ROW_RANDOM_SEED\"])\n",
    "randomize = False\n",
    "liteLLM_retries = int(os.environ[\"LITELLM_RETRIES\"])  # 15\n",
    "\n",
    "# Convert ROW_SAMPLE_SIZE to None if it's 'None'\n",
    "if ROW_SAMPLE_SIZE == \"None\":\n",
    "    ROW_SAMPLE_SIZE = None\n",
    "else:\n",
    "    ROW_SAMPLE_SIZE = int(ROW_SAMPLE_SIZE)\n",
    "\n",
    "corenlp_libs = os.environ[\"CORE_NLP_LIBS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e6efcc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "out_name = model_name\n",
    "out_name = out_name.replace(\".\", \"_\")\n",
    "out_name = Path(out_name).stem\n",
    "\n",
    "rg_model_name = rg_model_name.replace(\".\", \"_\")\n",
    "rg_model_name = Path(rg_model_name).stem\n",
    "\n",
    "# Error log file\n",
    "error_log = f\"{out_dir}/eval_reports_{folder_name}/unit_time_errors_{out_name}.txt\"\n",
    "wikidb_path = f\"{out_dir}/{folder_name}\"\n",
    "output_path = f\"{out_dir}/{folder_name}_{data_type}_{subset}\"\n",
    "\n",
    "# Path setup\n",
    "eval_reports_path = Path(f\"{out_dir}/eval_reports_{folder_name}\")\n",
    "eval_reports_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure lock directory exists\n",
    "output_lock_dir = f\"{out_dir}/locks\"  # Directory for file locks\n",
    "os.makedirs(output_lock_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5916be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load subset for faster experimentation. \"SEC_WikiDB subset unfiltered - all file types\" - The smaller 49 csv files for quick prototyping.\n",
    "if not run_from_localdir:\n",
    "    dataset = load_dataset(\n",
    "        \"ibm-research/struct-text\",\n",
    "        f\"{folder_name}_{data_type}_{subset}\",\n",
    "        streaming=False,\n",
    "        cache_dir=output_path,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d5ae17",
   "metadata": {},
   "source": [
    "# Model Configs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8261cf32",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f\"Model: {model_name}, Output name: {out_name}, RG model: {rg_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a236eb0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "completion_args = {\n",
    "    \"model\": f\"{LITELLM_MODEL}/{model_name}\",\n",
    "    \"api_base\": f\"{LLM_API_BASE}\",\n",
    "    \"temperature\": 0,\n",
    "    \"api_key\": API_KEY,\n",
    "    \"cache\": 0,\n",
    "    \"num_retries\": liteLLM_retries,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80734800",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "lm = dspy.LM(**completion_args)\n",
    "dspy.configure(lm=lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06471204",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "resp = lm(\"Ping\")\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c8b883",
   "metadata": {},
   "source": [
    "# Unit Time accuracy builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a32c6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from src.evaluation_utils import ensure_sutime_client, concretise_timex, ureg\n",
    "\n",
    "# spin up (only the first call actually starts JVM)\n",
    "sutime_client = ensure_sutime_client(corenlp_libs)\n",
    "\n",
    "doc = sutime_client.annotate(\"Delivery expected next Tuesday.\")\n",
    "for ent in doc.mentions:\n",
    "    print(ent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1975a75",
   "metadata": {},
   "source": [
    "## Unit Time Accuracy - Parsers and helpler funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb318082",
   "metadata": {},
   "outputs": [],
   "source": [
    "Numeric = Tuple[float, Optional[str]]  # (value, int)\n",
    "\n",
    "\n",
    "def _canonicalise(value: float, unit: Optional[str]) -> float:\n",
    "    \"\"\"coanonicalization with the pint package\"\"\"\n",
    "    if ureg and unit:\n",
    "        try:\n",
    "            q = value * ureg(unit)\n",
    "            return q.to_base_units().magnitude\n",
    "        except Exception:\n",
    "            pass\n",
    "    return value\n",
    "\n",
    "\n",
    "def _extract_numerics_with_ner(text: str) -> List[Numeric]:\n",
    "    \"\"\"Use the coreNLP to parse the named entity recognition types pertaining to the numeric\"\"\"\n",
    "    nums = []\n",
    "    doc_json = sutime_client.annotate(text, output_format=\"json\")\n",
    "\n",
    "    # Extract the Named entity recongition focussing on {\"NUMBER\", \"MONEY\", \"ORDINAL\", \"PERCENT\"}:\n",
    "    # REF: https://stanfordnlp.github.io/CoreNLP/ner.html#description\n",
    "    for s in doc_json[\"sentences\"]:\n",
    "        for em in s.get(\"entitymentions\", []):\n",
    "            if em[\"ner\"] in {\"NUMBER\", \"MONEY\", \"ORDINAL\", \"PERCENT\"}:\n",
    "                # regex my way out of this mess\n",
    "                text_clean = em[\"text\"].replace(\",\", \"\")\n",
    "                # Extract only the numeric part:\n",
    "                numeric_match = re.search(r\"[\\d.]+\", text_clean)\n",
    "                if numeric_match:\n",
    "                    value = float(numeric_match.group())\n",
    "                    nums.append((value, em[\"ner\"]))\n",
    "    return nums\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _extract_dates_with_sutime(text):\n",
    "    # Extract dates from text using SUTime via Stanford's CoreNLP.\n",
    "    # List to store extracted datetime objects\n",
    "    date_objects = []\n",
    "    try:\n",
    "        # Annotate the text\n",
    "        doc_json = sutime_client.annotate(text, output_format=\"json\")\n",
    "\n",
    "        # Extract TIMEX annotations, focusing on standard date formats\n",
    "        for s in doc_json[\"sentences\"]:\n",
    "            for em in s.get(\"entitymentions\", []):\n",
    "                if \"timex\" in em:\n",
    "                    t = em[\"timex\"]\n",
    "                    date_str = t[\"value\"]\n",
    "                    date_objects.append(\n",
    "                        {\"text\": em[\"text\"], \"value\": t[\"value\"], \"type\": t[\"type\"]}\n",
    "                    )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in SUTime date extraction: {e}\")\n",
    "\n",
    "    return date_objects\n",
    "\n",
    "\n",
    "def _convert_to_datetime(date_str: str) -> Optional[dt.datetime]:\n",
    "    try:\n",
    "        # Handle full ISO format dates\n",
    "        return dt.datetime.fromisoformat(date_str)\n",
    "    except ValueError:\n",
    "        if re.match(r\"^\\d{4}$\", date_str):\n",
    "            return dt.datetime(int(date_str), 1, 1)\n",
    "        # Handle year-month formats\n",
    "        elif re.match(r\"^\\d{4}-\\d{2}$\", date_str):\n",
    "            year, month = date_str.split(\"-\")\n",
    "            return dt.datetime(int(year), int(month), 1)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "def _calculate_summary_statistics(\n",
    "    numeric_metrics: List[Dict], temporal_metrics: List[Dict]\n",
    ") -> Dict[str, float]:\n",
    "    summary = {}\n",
    "    # Numeric summary statistics\n",
    "    if numeric_metrics:\n",
    "        num_precisions = [\n",
    "            m[\"precision\"] for m in numeric_metrics if m.get(\"precision\") is not None\n",
    "        ]\n",
    "        num_recalls = [\n",
    "            m[\"recall\"] for m in numeric_metrics if m.get(\"recall\") is not None\n",
    "        ]\n",
    "        num_f1s = [m[\"f1\"] for m in numeric_metrics if m.get(\"f1\") is not None]\n",
    "\n",
    "        summary.update(\n",
    "            {\n",
    "                \"numeric_mean_precision\": (\n",
    "                    float(pd.Series(num_precisions).mean()) if num_precisions else None\n",
    "                ),\n",
    "                \"numeric_mean_recall\": (\n",
    "                    float(pd.Series(num_recalls).mean()) if num_recalls else None\n",
    "                ),\n",
    "                \"numeric_mean_f1\": (\n",
    "                    float(pd.Series(num_f1s).mean()) if num_f1s else None\n",
    "                ),\n",
    "                \"numeric_evaluations_count\": len(numeric_metrics),\n",
    "            }\n",
    "        )\n",
    "    # Temporal summary statistics\n",
    "    if temporal_metrics:\n",
    "        tmp_precisions = [\n",
    "            m[\"precision\"] for m in temporal_metrics if m.get(\"precision\") is not None\n",
    "        ]\n",
    "        tmp_recalls = [\n",
    "            m[\"recall\"] for m in temporal_metrics if m.get(\"recall\") is not None\n",
    "        ]\n",
    "        tmp_f1s = [m[\"f1\"] for m in temporal_metrics if m.get(\"f1\") is not None]\n",
    "\n",
    "        summary.update(\n",
    "            {\n",
    "                \"temporal_mean_precision\": (\n",
    "                    float(pd.Series(tmp_precisions).mean()) if tmp_precisions else None\n",
    "                ),\n",
    "                \"temporal_mean_recall\": (\n",
    "                    float(pd.Series(tmp_recalls).mean()) if tmp_recalls else None\n",
    "                ),\n",
    "                \"temporal_mean_f1\": (\n",
    "                    float(pd.Series(tmp_f1s).mean()) if tmp_f1s else None\n",
    "                ),\n",
    "                \"temporal_evaluations_count\": len(temporal_metrics),\n",
    "            }\n",
    "        )\n",
    "    # Overall statistics\n",
    "    all_f1s = []\n",
    "    if numeric_metrics:\n",
    "        all_f1s.extend([m[\"f1\"] for m in numeric_metrics if m.get(\"f1\") is not None])\n",
    "    if temporal_metrics:\n",
    "        all_f1s.extend([m[\"f1\"] for m in temporal_metrics if m.get(\"f1\") is not None])\n",
    "\n",
    "    summary[\"overall_mean_f1\"] = float(pd.Series(all_f1s).mean()) if all_f1s else None\n",
    "    summary[\"total_evaluations\"] = len(numeric_metrics) + len(temporal_metrics)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4506560",
   "metadata": {},
   "source": [
    "## Main Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6be2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitTimeAccuracyEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,  # “Everything that comes after this must be passed by keyword, not by position.”\n",
    "        original_csv: pd.DataFrame,\n",
    "        plan_csv: pd.DataFrame,\n",
    "        generated_csv: pd.DataFrame,\n",
    "        numeric_tol: float = 1e-4,\n",
    "        temporal_tol_days: int = 0,\n",
    "        verbose: bool = True,\n",
    "        temporal_extractor: dspy.Signature,\n",
    "    ) -> None:\n",
    "        self.df_orig = original_csv\n",
    "        self.df_plan = plan_csv\n",
    "        self.df_gen = generated_csv\n",
    "\n",
    "        self.numeric_tol = numeric_tol\n",
    "        self.temporal_tol_days = temporal_tol_days\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Build the dspy:\n",
    "        self.temporal_extractor = dspy.Predict(temporal_extractor)\n",
    "\n",
    "        # build plan mapping:\n",
    "        self.plan_map: Dict[str, List[str]] = {\n",
    "            col: ast.literal_eval(self.df_plan.iloc[0, idx])\n",
    "            for idx, col in enumerate(self.df_plan.columns)\n",
    "        }\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def _compare_numeric(self, pred: float, truth: float) -> bool:\n",
    "        if truth == 0:\n",
    "            return abs(pred - truth) < self.numeric_tol\n",
    "        return abs(pred - truth) / abs(truth) <= self.numeric_tol\n",
    "\n",
    "    def _compare_temporal(self, pred_str: str, truth_str: str) -> bool:\n",
    "        # Direct string match for quarters\n",
    "        if \"-Q\" in pred_str and \"-Q\" in truth_str:\n",
    "            return pred_str == truth_str\n",
    "\n",
    "        pred_dt = _convert_to_datetime(pred_str)\n",
    "        truth_dt = _convert_to_datetime(truth_str)\n",
    "        # if self.verbose:\n",
    "        #     print(f\"Date time post parsing: Pred: {pred_dt} --> Truth: {truth_dt}\")\n",
    "        if not pred_dt or not truth_dt:\n",
    "            return False\n",
    "\n",
    "        delta = abs(pred_dt.date() - truth_dt.date())\n",
    "        # if self.verbose:\n",
    "        #     print(f\"delta: {delta}, pred: {pred_dt.date()}, truth: {truth_dt.date()}\")\n",
    "        #     print(\"--\" * 10)\n",
    "        return delta.days <= self.temporal_tol_days\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def _check_numeric_improved(\n",
    "        self, rep_type: str, text: str, row_gen: pd.Series\n",
    "    ) -> Tuple[Dict[str, float], List[str]]:\n",
    "        fails: List[str] = []\n",
    "        cols = self.plan_map.get(rep_type, [])\n",
    "        # extracted = _extract_numerics(text)\n",
    "        extracted = _extract_numerics_with_ner(text)\n",
    "        if self.verbose:\n",
    "            print(f\"Processing truth row: \\n{row_gen.get(cols)}\")\n",
    "            print(f\"Gen Txt: {text}\")\n",
    "            print(f\"Report type: {rep_type}\")\n",
    "            print(f\"Extracted nums: {extracted}\")\n",
    "            print(f\"Num of Extracted nums: {len(extracted)}\")\n",
    "            print(\"===\" * 3)\n",
    "\n",
    "        # Count valid ground truth numeric values\n",
    "        valid_ground_truth = []\n",
    "        for col in cols:\n",
    "            truth_raw = row_gen.get(col)\n",
    "            if pd.isna(truth_raw):\n",
    "                continue\n",
    "            try:\n",
    "                truth_val = float(truth_raw)\n",
    "                valid_ground_truth.append((col, truth_val))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        # Case 1: No ground truth numerics exist\n",
    "        if not valid_ground_truth:\n",
    "            if not extracted:\n",
    "                # correctly identified no numeric information\n",
    "                return {\n",
    "                    \"precision\": 1.0,\n",
    "                    \"recall\": 1.0,\n",
    "                    \"f1\": 1.0,\n",
    "                    \"true_positives\": 0,\n",
    "                    \"false_positives\": 0,\n",
    "                    \"false_negatives\": 0,\n",
    "                    \"interpretation\": \"correct_no_numerics\",\n",
    "                }, []\n",
    "            else:\n",
    "                # Error: Extractd numerics when none should exist.\n",
    "                # This is layering both the error from the pandas and maybe the way the number is represented.\n",
    "                # I recognize this but I think that's ok. Most of the spot checking did'nt trigger a false part here.\n",
    "                return {\n",
    "                    \"precision\": 0.0,  # 0 correct out of len(extracted) extractions\n",
    "                    \"recall\": None,  # undefined: can't miss what dosen't exist (TP/(TP+FN) = 0/(0+0) = 0/0)\n",
    "                    \"f1\": None,  # Cant calculate F1 when recall is undefined\n",
    "                    \"interpretation\": \"false_extractions_no_ground_truth\",\n",
    "                }, [f\"extracted {len(extracted)} numbers when none expected\"]\n",
    "        # Case 2: Ground truth numerics exist:\n",
    "        if not extracted:\n",
    "            # Error: Should have extracted but didn't\n",
    "            return {\n",
    "                \"precision\": None,  # Undefined: because no extractions to evaluate: (TP/(TP+FP)=0/(0+0) = undefined)\n",
    "                \"recall\": 0.0,  # Missed eveyrthing that existed\n",
    "                \"f1\": None,  # can't calulate f1 when precision is undefined.\n",
    "                \"true_positives\": 0,\n",
    "                \"extracted_count\": 0,\n",
    "                \"ground_truth_count\": len(valid_ground_truth),\n",
    "                \"interpretation\": \"no_extractions_made\",\n",
    "            }, [f\"made no extractions when {len(valid_ground_truth)} dates expected\"]\n",
    "\n",
    "        # case 3: both ground truth and extractions exist - standard eval:\n",
    "        used_extractions = set()\n",
    "        matched_truths = 0\n",
    "\n",
    "        for col, truth_val in valid_ground_truth:\n",
    "            truth_cannon = _canonicalise(truth_val, None)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Truth raw: {truth_val}, col: {col}\")\n",
    "                print(f\"Truth canonical: {truth_cannon}\")\n",
    "\n",
    "            found_match = False\n",
    "            for i, (pred_val, pred_unit) in enumerate(extracted):\n",
    "                if i in used_extractions:\n",
    "                    continue\n",
    "\n",
    "                pred_cannon = _canonicalise(pred_val, pred_unit)\n",
    "                if self._compare_numeric(pred_cannon, truth_cannon):\n",
    "                    used_extractions.add(i)\n",
    "                    matched_truths += 1\n",
    "                    found_match = True\n",
    "                    if self.verbose:\n",
    "                        print(\n",
    "                            f\"Matched Pred: {pred_cannon}, matched_truths_count: {matched_truths}\"\n",
    "                        )\n",
    "                        print(\"--\" * 15)\n",
    "\n",
    "                    break\n",
    "\n",
    "            if not found_match:\n",
    "                fails.append(f\"numeric mismatch: {col}\")\n",
    "\n",
    "        # Standard precisio/recall:\n",
    "        tp = matched_truths\n",
    "        fp = len(extracted) - len(used_extractions)\n",
    "        fn = len(valid_ground_truth) - matched_truths\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 1.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 1.0\n",
    "        f1 = (\n",
    "            2 * (precision * recall) / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0.0\n",
    "        )\n",
    "        return {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"true_positives\": tp,\n",
    "            \"false_positives\": fp,\n",
    "            \"false_negatives\": fn,\n",
    "            \"interpretation\": \"standard_evaluation\",\n",
    "        }, fails\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def _check_temporal_improved_with_llm(\n",
    "        self,\n",
    "        rep_type: str,\n",
    "        text: str,\n",
    "        row_gen: pd.Series,\n",
    "    ) -> Tuple[Dict[str, float], List[str], str]:\n",
    "        \"\"\"Modified with LLM pre-parsing to handle the date variablilty.\"\"\"\n",
    "        fails: List[str] = []\n",
    "        cols = self.plan_map.get(rep_type, [])\n",
    "\n",
    "        temporal_extractor = self.temporal_extractor\n",
    "        # Step 1: Use LLM to extract temporal information\n",
    "        # instaed of using SUTime directly we use the LLM first:\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Using LLM to extract temporal info from text...\")\n",
    "            print(f\"Processing truth row: \\n{row_gen.get(cols)}\")\n",
    "            print(f\"Gen Txt: {text}\")\n",
    "\n",
    "        try:\n",
    "            result = temporal_extractor(text=text)\n",
    "            # Parse the json output from DSPy with pydantic verification.\n",
    "            extracted_dspy = result.temporal_json\n",
    "            ################ Extract normalized text ###########\n",
    "            extracted_dates = []\n",
    "            for ed in extracted_dspy:\n",
    "                ed = (\n",
    "                    ed.model_dump()\n",
    "                )  # since this is a pydantic output to use it as a dict you need this line.\n",
    "                if \"normalized_value\" in ed:\n",
    "                    extracted_dates.append(\n",
    "                        {\n",
    "                            \"value\": ed[\"normalized_value\"],\n",
    "                            \"type\": ed.get(\"type\", \"unknown\"),\n",
    "                            \"text\": ed.get(\"original_text\", \"\"),\n",
    "                        }\n",
    "                    )\n",
    "            if not extracted_dates:  # empty list\n",
    "                if self.verbose:\n",
    "                    print(f\"DSPy found no dates, falling back to SUTime\")\n",
    "                extracted_dates = _extract_dates_with_sutime(text)\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to parse DSPy JSON output: {e}\")\n",
    "            print(f\"Raw output: {result.temporal_json}\")\n",
    "            print(\"ErErErEr\" * 20)\n",
    "            # Show the exact representation with all hidden characters\n",
    "            print(f\"Raw output repr: {repr(result.temporal_json)}\")\n",
    "\n",
    "            extracted_dates = []\n",
    "        except Exception as e:\n",
    "            print(f\"DSPy temporal extraction failed: {e}\")\n",
    "            # Fallback to SUTime\n",
    "            # extracted_dates = []\n",
    "            extracted_dates = _extract_dates_with_sutime(text)\n",
    "\n",
    "        ##########################################################\n",
    "        masked_out = text\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Number of extracted dates: {len(extracted_dates)}\")\n",
    "            print(f\"Extracted Dates: {extracted_dates}\")\n",
    "\n",
    "        # %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        # STEP 3: Process ground truth\n",
    "        valid_ground_truth = []\n",
    "        for col in cols:\n",
    "            truth_raw = row_gen.get(col)\n",
    "            if pd.isna(truth_raw):\n",
    "                continue\n",
    "\n",
    "            parsed_result = None\n",
    "            # First use SUTime for truth parsing. Speeds up processing.\n",
    "            try:\n",
    "                truth_date = _extract_dates_with_sutime(str(truth_raw))\n",
    "                if self.verbose:\n",
    "                    print(f\"Truth value for {col}: {truth_raw} -> {truth_date}\")\n",
    "\n",
    "                # Check if SUTime found anything useful\n",
    "                if truth_date and truth_date[0][\"type\"] in (\"DATE\", \"TIME\"):\n",
    "                    parsed_result = (col, truth_date[0][\"value\"], truth_raw)\n",
    "            except Exception as e:\n",
    "                # SUTime failed or returned empty - check if we should try DSPy\n",
    "                print(f\"SUTime failed or returned empty for: {e}\")\n",
    "\n",
    "                try:\n",
    "                    result = temporal_extractor(text=f\"Extract date from: {truth_raw}\")\n",
    "                    dspy_truth = json.loads(result.temporal_json)\n",
    "                    if dspy_truth and dspy_truth[0].get(\"normalized_value\"):\n",
    "                        parsed_result = (\n",
    "                            col,\n",
    "                            dspy_truth[0][\"normalized_value\"],\n",
    "                            truth_raw,\n",
    "                        )\n",
    "                        if self.verbose:\n",
    "                            print(\n",
    "                                f\"DSPy extracted from truth: {dspy_truth[0]['normalized_value']}\"\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    if self.verbose:\n",
    "                        print(f\"DSPy didn't find temporal in '{truth_raw}': {e}\")\n",
    "\n",
    "            if parsed_result:\n",
    "                valid_ground_truth.append(parsed_result)\n",
    "\n",
    "        # Evaluation cases:\n",
    "        if not valid_ground_truth:\n",
    "            if not extracted_dates:\n",
    "                return (\n",
    "                    {\n",
    "                        \"precision\": 1.0,\n",
    "                        \"recall\": 1.0,\n",
    "                        \"f1\": 1.0,\n",
    "                        \"interpretation\": \"correct_no_temporal\",\n",
    "                    },\n",
    "                    [],\n",
    "                    masked_out,\n",
    "                )\n",
    "            else:\n",
    "                return (\n",
    "                    {\n",
    "                        \"precision\": 0.0,\n",
    "                        \"recall\": None,\n",
    "                        \"f1\": None,\n",
    "                        \"interpretation\": \"false_temporal_extractions\",\n",
    "                    },\n",
    "                    [\n",
    "                        f\"extracted {len(extracted_dates)} temporal elements when none expected\"\n",
    "                    ],\n",
    "                    masked_out,\n",
    "                )\n",
    "\n",
    "        if not extracted_dates:\n",
    "            return (\n",
    "                {\n",
    "                    \"precision\": None,\n",
    "                    \"recall\": 0.0,\n",
    "                    \"f1\": None,\n",
    "                    \"interpretation\": \"missed_all_temporal\",\n",
    "                },\n",
    "                [f\"missed {len(valid_ground_truth)} temporal elements\"],\n",
    "                masked_out,\n",
    "            )\n",
    "\n",
    "        # Match extracted vs truth\n",
    "        matches_per_extraction = {}\n",
    "        matched_truths = 0\n",
    "        # print(\"--\" * 10)\n",
    "        for col, truth_date_obj, truth_raw in valid_ground_truth:\n",
    "            if self.verbose:\n",
    "                print(f\"Comparing truth: {col}: {truth_raw} -> {truth_date_obj}\")\n",
    "\n",
    "            found_match = False\n",
    "            for i, extracted_date in enumerate(extracted_dates):\n",
    "                # if i in used_extractions:\n",
    "                #     continue\n",
    "                # NO SKIP for extractions!!!!\n",
    "\n",
    "                extracted_date_obj = extracted_date[\"value\"]\n",
    "                extracted_date_obj = concretise_timex(extracted_date_obj)\n",
    "\n",
    "                if self._compare_temporal(extracted_date_obj, truth_date_obj):\n",
    "                    # used_extractions.add(i)\n",
    "                    matched_truths += 1\n",
    "                    found_match = True\n",
    "\n",
    "                    # Track usage for stats:\n",
    "                    matches_per_extraction[i] = matches_per_extraction.get(i, 0) + 1\n",
    "\n",
    "                    if self.verbose:\n",
    "                        print(f\"Matched: {extracted_date_obj} -> {truth_date_obj}\")\n",
    "                    break\n",
    "\n",
    "            if not found_match:\n",
    "                fails.append(f\"date mismatch: {col}={truth_raw}\")\n",
    "\n",
    "        # Calculate metrics\n",
    "        tp = matched_truths\n",
    "        # fp = len(extracted_dates) - len(used_extractions)\n",
    "        fp = len(\n",
    "            [i for i in range(len(extracted_dates)) if i not in matches_per_extraction]\n",
    "        )\n",
    "        fn = len(valid_ground_truth) - matched_truths\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 1.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 1.0\n",
    "        f1 = (\n",
    "            2 * (precision * recall) / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            {\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1,\n",
    "                \"true_positives\": tp,\n",
    "                \"false_positives\": fp,\n",
    "                \"false_negatives\": fn,\n",
    "                \"extracted_count\": len(extracted_dates),\n",
    "                \"ground_truth_count\": len(valid_ground_truth),\n",
    "                \"interpretation\": \"asymmetric_parsing\",\n",
    "            },\n",
    "            fails,\n",
    "            masked_out,\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def _structure_fails(\n",
    "        self, num_fails: List[str], tmp_fails: List[str], rep_type: str\n",
    "    ) -> Dict[str, object]:\n",
    "        total_fails = len(num_fails) + len(tmp_fails)\n",
    "\n",
    "        fail_summary = []\n",
    "        if num_fails:\n",
    "            fail_summary.append(f\"numeric({len(num_fails)})\")\n",
    "        if tmp_fails:\n",
    "            fail_summary.append(f\"temporal({len(tmp_fails)})\")\n",
    "\n",
    "        return {\n",
    "            \"total_count\": total_fails,\n",
    "            \"numeric\": \"; \".join(num_fails) if num_fails else None,\n",
    "            \"temporal\": \"; \".join(tmp_fails) if tmp_fails else None,\n",
    "            \"summary\": \"; \".join(fail_summary) if fail_summary else \"no_fails\",\n",
    "        }\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def run(self) -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
    "        results: List[Dict[str, object]] = []\n",
    "\n",
    "        all_numeric_metrics = []\n",
    "        all_temporal_metrics = []\n",
    "        for idx, row in self.df_gen.iterrows():\n",
    "            truth_row = self.df_orig.iloc[idx]\n",
    "\n",
    "            for rep_type, text in row.items():\n",
    "                if isinstance(text, float) and math.isnan(text):\n",
    "                    continue\n",
    "\n",
    "                cols = self.plan_map.get(rep_type, [])\n",
    "                if self.verbose:\n",
    "                    print(f\"Num Truth cols: {len(cols)}\")\n",
    "                    print(f\"Truth cols: {truth_row.get(cols)}\")\n",
    "                # Run both evaluation methods\n",
    "                num_metrics, num_fails = self._check_numeric_improved(\n",
    "                    rep_type, text, truth_row\n",
    "                )\n",
    "                tmp_metrics, tmp_fails, _ = self._check_temporal_improved_with_llm(\n",
    "                    rep_type, text, truth_row\n",
    "                )\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(\"xxx\" * 10)\n",
    "                    print(f\"Numeric for Rep type: {rep_type}: {num_metrics}\")\n",
    "                    print(f\"Temporal for Rep type: {rep_type}: {tmp_metrics}\")\n",
    "\n",
    "                # Structure the fails more clearly\n",
    "                structured_fails = self._structure_fails(num_fails, tmp_fails, rep_type)\n",
    "                # Collect metrics for summary (only if we have valid data)\n",
    "                if num_metrics and isinstance(num_metrics, dict):\n",
    "                    all_numeric_metrics.append(num_metrics)\n",
    "                if (\n",
    "                    tmp_metrics\n",
    "                    and isinstance(tmp_metrics, dict)\n",
    "                    and tmp_metrics.get(\"precision\") is not None\n",
    "                ):\n",
    "                    all_temporal_metrics.append(tmp_metrics)\n",
    "\n",
    "                # Build the result row\n",
    "                result_row = {\n",
    "                    \"row_idx\": idx,\n",
    "                    \"report_type\": rep_type,\n",
    "                    \"generated_text\": (\n",
    "                        text[:100] + \"...\" if len(str(text)) > 100 else text\n",
    "                    ),  # Truncate for readability\n",
    "                    # Numeric metrics:\n",
    "                    \"num_precision\": (\n",
    "                        num_metrics.get(\"precision\", None) if num_metrics else None\n",
    "                    ),\n",
    "                    \"num_recall\": (\n",
    "                        num_metrics.get(\"recall\", None) if num_metrics else None\n",
    "                    ),\n",
    "                    \"num_f1\": num_metrics.get(\"f1\", None) if num_metrics else None,\n",
    "                    \"num_tp\": (\n",
    "                        num_metrics.get(\"true_positives\", None) if num_metrics else None\n",
    "                    ),\n",
    "                    \"num_fp\": (\n",
    "                        num_metrics.get(\"false_positives\", None)\n",
    "                        if num_metrics\n",
    "                        else None\n",
    "                    ),\n",
    "                    \"num_fn\": (\n",
    "                        num_metrics.get(\"false_negatives\", None)\n",
    "                        if num_metrics\n",
    "                        else None\n",
    "                    ),\n",
    "                    \"num_interpretation\": (\n",
    "                        num_metrics.get(\"interpretation\", None) if num_metrics else None\n",
    "                    ),\n",
    "                    # Temporal metrics\n",
    "                    \"tmp_precision\": (\n",
    "                        tmp_metrics.get(\"precision\", None) if tmp_metrics else None\n",
    "                    ),\n",
    "                    \"tmp_recall\": (\n",
    "                        tmp_metrics.get(\"recall\", None) if tmp_metrics else None\n",
    "                    ),\n",
    "                    \"tmp_f1\": tmp_metrics.get(\"f1\", None) if tmp_metrics else None,\n",
    "                    \"tmp_tp\": (\n",
    "                        tmp_metrics.get(\"true_positives\", None) if tmp_metrics else None\n",
    "                    ),\n",
    "                    \"tmp_extracted_count\": (\n",
    "                        tmp_metrics.get(\"extracted_count\", None)\n",
    "                        if tmp_metrics\n",
    "                        else None\n",
    "                    ),\n",
    "                    \"tmp_ground_truth_count\": (\n",
    "                        tmp_metrics.get(\"ground_truth_count\", None)\n",
    "                        if tmp_metrics\n",
    "                        else None\n",
    "                    ),\n",
    "                    \"tmp_interpreation\": (\n",
    "                        tmp_metrics.get(\"interpretation\", None) if tmp_metrics else None\n",
    "                    ),\n",
    "                    # Failure analysis\n",
    "                    \"fail_count\": structured_fails[\"total_count\"],\n",
    "                    \"numeric_fails\": structured_fails[\"numeric\"],\n",
    "                    \"temporal_fails\": structured_fails[\"temporal\"],\n",
    "                    \"fail_summary\": structured_fails[\"summary\"],\n",
    "                    # Placeholder for LLM evaluation (you can enable this later)\n",
    "                    \"llm_verified\": None,  # TODO: Implement LLM verification\n",
    "                }\n",
    "                results.append(result_row)\n",
    "                if self.verbose:\n",
    "                    print(\"==\" * 10)\n",
    "\n",
    "        # Create the results DataFrame\n",
    "        df_results = pd.DataFrame(results)\n",
    "        # Calculate summary statistics\n",
    "        summary_stats = _calculate_summary_statistics(\n",
    "            all_numeric_metrics, all_temporal_metrics\n",
    "        )\n",
    "        # summary_stats = []\n",
    "        return df_results, summary_stats\n",
    "\n",
    "    def parallel_run(self, log_file) -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
    "        \"\"\"Parallel version of run() - will replace the original\"\"\"\n",
    "        results: List[Dict[str, object]] = []\n",
    "        all_numeric_metrics = []\n",
    "        all_temporal_metrics = []\n",
    "\n",
    "        # Process single row-report combination:\n",
    "        def process_item(args):\n",
    "            idx, rep_type, text, truth_row = args\n",
    "            with capture_row():\n",
    "                ROW_LOG.info(\"row-%d : start\", idx)\n",
    "                if isinstance(text, float) and math.isnan(text):\n",
    "                    return None\n",
    "\n",
    "                try:\n",
    "\n",
    "                    num_metrics, num_fails = self._check_numeric_improved(\n",
    "                        rep_type, text, truth_row\n",
    "                    )\n",
    "                    tmp_metrics, tmp_fails, masked_out = (\n",
    "                        self._check_temporal_improved_with_llm(\n",
    "                            rep_type, text, truth_row\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # Structre that fails:\n",
    "                    structured_fails = self._structure_fails(\n",
    "                        num_fails, tmp_fails, rep_type\n",
    "                    )\n",
    "\n",
    "                    # build the result row (same as the serial one)\n",
    "                    result_row = {\n",
    "                        \"row_idx\": idx,\n",
    "                        \"report_type\": rep_type,\n",
    "                        \"generated_text\": (\n",
    "                            text[:100] + \"...\" if len(str(text)) > 100 else text\n",
    "                        ),\n",
    "                        # Numeric metrics\n",
    "                        \"num_precision\": (\n",
    "                            num_metrics.get(\"precision\", None) if num_metrics else None\n",
    "                        ),\n",
    "                        \"num_recall\": (\n",
    "                            num_metrics.get(\"recall\", None) if num_metrics else None\n",
    "                        ),\n",
    "                        \"num_f1\": num_metrics.get(\"f1\", None) if num_metrics else None,\n",
    "                        \"num_tp\": (\n",
    "                            num_metrics.get(\"true_positives\", None)\n",
    "                            if num_metrics\n",
    "                            else None\n",
    "                        ),\n",
    "                        \"num_fp\": (\n",
    "                            num_metrics.get(\"false_positives\", None)\n",
    "                            if num_metrics\n",
    "                            else None\n",
    "                        ),\n",
    "                        \"num_fn\": (\n",
    "                            num_metrics.get(\"false_negatives\", None)\n",
    "                            if num_metrics\n",
    "                            else None\n",
    "                        ),\n",
    "                        \"num_interpretation\": (\n",
    "                            num_metrics.get(\"interpretation\", None)\n",
    "                            if num_metrics\n",
    "                            else None\n",
    "                        ),\n",
    "                        # Temporal metrics\n",
    "                        \"tmp_precision\": (\n",
    "                            tmp_metrics.get(\"precision\", None) if tmp_metrics else None\n",
    "                        ),\n",
    "                        \"tmp_recall\": (\n",
    "                            tmp_metrics.get(\"recall\", None) if tmp_metrics else None\n",
    "                        ),\n",
    "                        \"tmp_f1\": tmp_metrics.get(\"f1\", None) if tmp_metrics else None,\n",
    "                        \"tmp_tp\": (\n",
    "                            tmp_metrics.get(\"true_positives\", None)\n",
    "                            if tmp_metrics\n",
    "                            else None\n",
    "                        ),\n",
    "                        \"tmp_extracted_count\": (\n",
    "                            tmp_metrics.get(\"extracted_count\", None)\n",
    "                            if tmp_metrics\n",
    "                            else None\n",
    "                        ),\n",
    "                        \"tmp_ground_truth_count\": (\n",
    "                            tmp_metrics.get(\"ground_truth_count\", None)\n",
    "                            if tmp_metrics\n",
    "                            else None\n",
    "                        ),\n",
    "                        \"tmp_interpreation\": (\n",
    "                            tmp_metrics.get(\"interpretation\", None)\n",
    "                            if tmp_metrics\n",
    "                            else None\n",
    "                        ),\n",
    "                        # Failure analysis\n",
    "                        \"fail_count\": structured_fails[\"total_count\"],\n",
    "                        \"numeric_fails\": structured_fails[\"numeric\"],\n",
    "                        \"temporal_fails\": structured_fails[\"temporal\"],\n",
    "                        \"fail_summary\": structured_fails[\"summary\"],\n",
    "                        \"llm_verified\": None,\n",
    "                    }\n",
    "\n",
    "                    ROW_LOG.info(\"row-%d : done\", idx)\n",
    "\n",
    "                    return result_row, num_metrics, tmp_metrics\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row {idx}, report {rep_type}: {str(e)}\")\n",
    "                    if self.verbose:\n",
    "                        import traceback\n",
    "\n",
    "                        traceback.print_exc()\n",
    "                    return None\n",
    "\n",
    "        # Prepare all work items\n",
    "        work_items = []\n",
    "        for idx, row in self.df_gen.iterrows():\n",
    "            truth_row = self.df_orig.iloc[idx]\n",
    "            for rep_type, text in row.items():\n",
    "                work_items.append((idx, rep_type, text, truth_row))\n",
    "        # Now process in paralale:\n",
    "        print(f\"Processing {len(work_items)} items with {max_workers} workers ...\")\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # # submit all the jobs:\n",
    "            # futures = [executor.submit(process_item, item) for item in work_items]\n",
    "\n",
    "            # # Collect the results with progress bar:\n",
    "            # for future in tqdm(\n",
    "            #     as_completed(futures), total=len(futures), desc=\"processing\"\n",
    "            # ):\n",
    "            ###########################################################################\n",
    "            ## Use Pool.map i.e. parallel run but sequentail write so you get the output in the same order you inputted\n",
    "            results_iter = executor.map(process_item, work_items)\n",
    "            for i, result in enumerate(\n",
    "                tqdm(results_iter, total=len(work_items), desc=\"processing\")\n",
    "            ):\n",
    "                try:\n",
    "                    # result = future.result()\n",
    "                    if result is not None:\n",
    "\n",
    "                        result_row, num_metrics, tmp_metrics = result\n",
    "\n",
    "                        results.append(result_row)\n",
    "                        if num_metrics and isinstance(num_metrics, dict):\n",
    "                            all_numeric_metrics.append(num_metrics)\n",
    "                        if (\n",
    "                            tmp_metrics\n",
    "                            and isinstance(tmp_metrics, dict)\n",
    "                            and tmp_metrics.get(\"precision\") is not None\n",
    "                        ):\n",
    "                            all_temporal_metrics.append(tmp_metrics)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Future failed: {e}\")\n",
    "\n",
    "            # Sort results to maintain order:\n",
    "            results.sort(key=lambda x: (x[\"row_idx\"], x[\"report_type\"]))\n",
    "\n",
    "            df_results = pd.DataFrame(results)\n",
    "\n",
    "            summary_stats = _calculate_summary_statistics(\n",
    "                all_numeric_metrics, all_temporal_metrics\n",
    "            )\n",
    "            return df_results, summary_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4cfb5e",
   "metadata": {},
   "source": [
    "# Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace8f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_unit_time_module(\n",
    "    dataset_name,\n",
    "    ground_truth_data,\n",
    "    report_types_data,\n",
    "    generated_reports_data,\n",
    "    split_type,\n",
    "    log_filename,\n",
    "    rg_model_name,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Process a single dataset for LLM temporal numeric evaluation.\"\"\"\n",
    "\n",
    "    meta_csv_path = (\n",
    "        Path(output_path).parent / f\"meta_data_{rg_model_name}_{folder_name}.csv\"\n",
    "    )\n",
    "    with filelock.FileLock(f\"{output_lock_dir}/metadata.lock\"):\n",
    "        if run_from_localdir:\n",
    "            meta_csv_path = (\n",
    "                Path(output_path).parent\n",
    "                / f\"meta_data_{rg_model_name}_{folder_name}.csv\"\n",
    "            )\n",
    "            # Read metadata\n",
    "            if not meta_csv_path.exists():\n",
    "                raise FileNotFoundError(f\"Metadata file not found at {meta_csv_path}\")\n",
    "            meta_df = pd.read_csv(meta_csv_path)\n",
    "\n",
    "        else:\n",
    "            meta_ds = load_dataset(\n",
    "                \"ibm-research/struct-text\",\n",
    "                data_files=f\"meta_data_{rg_model_name}_{folder_name}.csv\",\n",
    "            )\n",
    "            meta_df = meta_ds[\"train\"].to_pandas()\n",
    "            meta_df.to_csv(meta_csv_path, index=False)\n",
    "            # And change the rg_model_name to the default: rg_model_name = \"Qwen2_5-72B-Instruct\"\n",
    "\n",
    "    try:\n",
    "        # Get the row for this dataset\n",
    "        dataset_row = meta_df[meta_df[\"dataset_name\"] == dataset_name]\n",
    "        if len(dataset_row) == 0:\n",
    "            raise ValueError(f\"Dataset '{dataset_name}' not found in metadata.\")\n",
    "        if len(dataset_row) > 1:\n",
    "            raise ValueError(\n",
    "                f\"Multiple entries for dataset '{dataset_name}' in metadata.\"\n",
    "            )\n",
    "\n",
    "        # Get the single row\n",
    "        row = dataset_row.iloc[0]\n",
    "        # Define output path and column\n",
    "        column_name = f\"unit_time_csv_path_{out_name}\"\n",
    "        column_name_summary = f\"unit_time_summary_{out_name}\"\n",
    "\n",
    "        output_csv = Path(eval_reports_path) / split_type\n",
    "        Path(output_csv).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        output_csv = (\n",
    "            output_csv / f\"{dataset_name}_{rg_model_name}_unit_time_{out_name}.csv\"\n",
    "        )\n",
    "\n",
    "        # Skip if already processed\n",
    "        existing_path = row.get(column_name, pd.NA)\n",
    "        if (\n",
    "            not force_reprocess\n",
    "            and not pd.isna(existing_path)\n",
    "            and os.path.exists(existing_path)\n",
    "            and os.path.getsize(existing_path) > 0\n",
    "        ):\n",
    "            print(f\"Skipping {dataset_name} - already processed\")\n",
    "            return\n",
    "\n",
    "        if run_from_localdir:\n",
    "            # Read data\n",
    "            original_csv = row[\"ground_truth_csv_path\"]\n",
    "            planned_csv = row[\"report_types_csv_path\"]\n",
    "            generated_csv = row[\"generated_reports_csv_path\"]\n",
    "\n",
    "            print(f\"Processing {dataset_name}\")\n",
    "            print(f\"Loading data from: {original_csv}, {planned_csv}, {generated_csv}\")\n",
    "\n",
    "            orig_df = pd.read_csv(original_csv)\n",
    "            planned_df = pd.read_csv(planned_csv)\n",
    "            generated_df = pd.read_csv(generated_csv)\n",
    "            row_plan = planned_df.iloc[0]\n",
    "        else:\n",
    "            # process from the hf datasets. The dataset_row should have that info:\n",
    "            orig_df = pd.read_csv(io.StringIO(ground_truth_data[\"csv_text\"]))\n",
    "            planned_df = pd.read_csv(io.StringIO(report_types_data[\"csv_text\"]))\n",
    "            generated_df = pd.read_csv(io.StringIO(generated_reports_data[\"csv_text\"]))\n",
    "            row_plan = planned_df.iloc[0]\n",
    "\n",
    "        # Sample rows if requested:\n",
    "        total_rows = len(generated_df)\n",
    "        if ROW_SAMPLE_SIZE is not None and total_rows > ROW_SAMPLE_SIZE:\n",
    "            # use fixed seed for reproducible sampling unique to the dataset as well.\n",
    "            # apparently random uses a hashmap and can take in unique strings as well\n",
    "            random.seed(f\"{ROW_RANDOM_SEED}_{dataset_name}\")\n",
    "            if randomize:\n",
    "                sampled_indices = random.sample(range(total_rows), ROW_SAMPLE_SIZE)\n",
    "                sampled_indices.sort()\n",
    "            else:\n",
    "                sampled_indices = list(range(0, ROW_SAMPLE_SIZE))\n",
    "\n",
    "            orig_df_sampled = orig_df.iloc[sampled_indices].reset_index(drop=True)\n",
    "            generated_df_sampled = generated_df.iloc[sampled_indices].reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "            print(f\"Sampled {len(sampled_indices)} rows from {total_rows} total rows\")\n",
    "        else:\n",
    "            # Use all the rows\n",
    "            orig_df_sampled = orig_df\n",
    "            generated_df_sampled = generated_df\n",
    "            sampled_indices = list(range(total_rows))\n",
    "            print(f\"Processing all {total_rows} rows (no sampling)\")\n",
    "\n",
    "        evaluator = UnitTimeAccuracyEvaluator(\n",
    "            original_csv=orig_df_sampled,\n",
    "            plan_csv=planned_df,\n",
    "            generated_csv=generated_df_sampled,\n",
    "            numeric_tol=1e-3,\n",
    "            temporal_tol_days=3,\n",
    "            temporal_extractor=TemporalExtractor,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        if exec_parallel_run:\n",
    "            report_scores, summary = evaluator.parallel_run(log_filename)\n",
    "        else:\n",
    "            report_scores, summary = evaluator.run()\n",
    "\n",
    "        report_scores.to_csv(output_csv, index=False)\n",
    "        print(f\"Results saved to {output_csv}\")\n",
    "\n",
    "        # Update metadata with a file lock to prevent race conditions\n",
    "        with filelock.FileLock(f\"{output_lock_dir}/metadata.lock\"):\n",
    "            # Re-read metadata to get the latest version\n",
    "            meta_df = pd.read_csv(meta_csv_path)\n",
    "            # Ensure the dataset still exists\n",
    "            if dataset_name not in meta_df[\"dataset_name\"].values:\n",
    "                print(\n",
    "                    f\"Warning: Dataset {dataset_name} no longer in metadata. Skipping update.\"\n",
    "                )\n",
    "                return\n",
    "\n",
    "            # Find the row again (index might have changed)\n",
    "            row_idx = meta_df[meta_df[\"dataset_name\"] == dataset_name].index[0]\n",
    "            if column_name_summary not in meta_df.columns:\n",
    "                meta_df[column_name_summary] = None  # Initialize with None for all rows\n",
    "\n",
    "            # Update the metadata\n",
    "            meta_df.at[row_idx, column_name] = str(output_csv)\n",
    "            meta_df.at[row_idx, column_name_summary] = str(summary)\n",
    "\n",
    "            # Save the metadata\n",
    "            meta_df.to_csv(meta_csv_path, index=False)\n",
    "    finally:\n",
    "        # ALways restor stdout and close file:\n",
    "        # sys.stdout = original_stdout\n",
    "        # log_file.close()\n",
    "        print(f\"Debug log saved to : {log_filename}\")\n",
    "    return evaluator, report_scores, summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f6e230",
   "metadata": {},
   "source": [
    "# Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a989128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\n",
    "    \"ground_truth\",\n",
    "    \"report_types\",\n",
    "    \"generated_reports\",\n",
    "]  # Hardcoded: See HF ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94272f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_from_localdir:\n",
    "    splits = {\n",
    "        \"train\": dataset[\"train\"],\n",
    "        \"val\": dataset[\"validation\"],\n",
    "        \"test\": dataset[\"test\"],\n",
    "    }\n",
    "    # this is the only run available on hf.\n",
    "    rg_model_name = \"Qwen2_5-72B-Instruct\"\n",
    "\n",
    "    for split_type, split_data in splits.items():\n",
    "        # create a dictionary to group files by dataset_name:\n",
    "        dataset_groups = defaultdict(\n",
    "            lambda: {\n",
    "                \"ground_truth\": None,\n",
    "                \"report_types\": None,\n",
    "                \"generated_reports\": None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # First split by all files by datast name:\n",
    "        for idx, split_row in enumerate(split_data):\n",
    "            row_label = label_names[split_row[\"report_type\"]]\n",
    "            file_name = split_row[\"file_name\"]\n",
    "\n",
    "            # extract the dataset name based on the file type:\n",
    "            if \"_ground_truth.csv\" in file_name:\n",
    "                dataset_name = file_name.replace(\"_ground_truth.csv\", \"\")\n",
    "                dataset_groups[dataset_name][\"ground_truth\"] = (idx, split_row)\n",
    "\n",
    "            if \"_report_types_\" in file_name:\n",
    "                dataset_name = file_name.replace(\n",
    "                    f\"_report_types_{rg_model_name}.csv\", \"\"\n",
    "                )\n",
    "                dataset_groups[dataset_name][\"report_types\"] = (idx, split_row)\n",
    "\n",
    "            if \"_generated_reports_\" in file_name:\n",
    "                dataset_name = file_name.replace(\n",
    "                    f\"_generated_reports_{rg_model_name}.csv\", \"\"\n",
    "                )\n",
    "                dataset_groups[dataset_name][\"generated_reports\"] = (idx, split_row)\n",
    "\n",
    "        ######################################################################################\n",
    "        # now process with each dataset group with all three files:\n",
    "        for dataset_name, files in dataset_groups.items():\n",
    "\n",
    "            log_filename = f\"{eval_reports_path}/{split_type}/{dataset_name}_{rg_model_name}_unit_time_{out_name}_debug.log\"\n",
    "            listener, print = setup_logging(log_filename, output_stream_to_console)\n",
    "\n",
    "            if all(files.values()):\n",
    "                print(f\"\\nProcessing dataset: {dataset_name} in {split_type}\")\n",
    "                # Extract the data for all three files\n",
    "                ground_truth_data = files[\"ground_truth\"][1]\n",
    "                report_types_data = files[\"report_types\"][1]\n",
    "                generated_reports_data = files[\"generated_reports\"][1]\n",
    "\n",
    "                print(ground_truth_data[\"file_name\"], ground_truth_data[\"report_type\"])\n",
    "                print(report_types_data[\"file_name\"], report_types_data[\"report_type\"])\n",
    "                print(\n",
    "                    generated_reports_data[\"file_name\"],\n",
    "                    generated_reports_data[\"report_type\"],\n",
    "                )\n",
    "\n",
    "                process_unit_time_module(\n",
    "                    dataset_name,\n",
    "                    ground_truth_data,\n",
    "                    report_types_data,\n",
    "                    generated_reports_data,\n",
    "                    split_type,\n",
    "                    log_filename,\n",
    "                    rg_model_name,\n",
    "                    verbose=is_verbose,\n",
    "                )\n",
    "\n",
    "            listener.stop()\n",
    "\n",
    "else:\n",
    "    for split_type in [\"train\", \"val\", \"test\"]:\n",
    "        # for split_type in [\"val\", \"test\"]:\n",
    "        split_dir = os.path.join(output_path, split_type)\n",
    "        if not os.path.exists(split_dir):\n",
    "            print(f\"Split directory {split_dir} does not exist. Skipping {split_type}.\")\n",
    "            continue\n",
    "\n",
    "        # create a dictionary to group files by dataset_name:\n",
    "        dataset_groups = defaultdict(\n",
    "            lambda: {\n",
    "                \"ground_truth\": None,\n",
    "                \"report_types\": None,\n",
    "                \"generated_reports\": None,\n",
    "            }\n",
    "        )\n",
    "        # List all CSV files in the split directory\n",
    "        for file_name in os.listdir(split_dir):\n",
    "            if not file_name.endswith(\".csv\"):\n",
    "                continue\n",
    "\n",
    "            full_path = os.path.join(split_dir, file_name)\n",
    "            with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                csv_text = f.read()\n",
    "\n",
    "            # extract the dataset name based on the file type:\n",
    "            if \"_ground_truth.csv\" in file_name:\n",
    "                dataset_name = file_name.replace(\"_ground_truth.csv\", \"\")\n",
    "                dataset_groups[dataset_name][\"ground_truth\"] = {\n",
    "                    \"file_name\": file_name,\n",
    "                    \"report_type\": 0,  # Dummy value\n",
    "                }\n",
    "\n",
    "            if \"_report_types_\" in file_name:\n",
    "                dataset_name = file_name.replace(\n",
    "                    f\"_report_types_{rg_model_name}.csv\", \"\"\n",
    "                )\n",
    "                dataset_groups[dataset_name][\"report_types\"] = {\n",
    "                    \"file_name\": file_name,\n",
    "                    \"report_type\": 1,  # Dummy value\n",
    "                }\n",
    "\n",
    "            if \"_generated_reports_\" in file_name:\n",
    "                dataset_name = file_name.replace(\n",
    "                    f\"_generated_reports_{rg_model_name}.csv\", \"\"\n",
    "                )\n",
    "                dataset_groups[dataset_name][\"generated_reports\"] = {\n",
    "                    \"file_name\": file_name,\n",
    "                    \"report_type\": 2,  # Dummy value\n",
    "                }\n",
    "        ######################################################################################\n",
    "        # now process each dataset group with all three files:\n",
    "        for dataset_name, files in dataset_groups.items():\n",
    "            log_filename = f\"{eval_reports_path}/{split_type}/{dataset_name}_{rg_model_name}_unit_time_{out_name}_debug.log\"\n",
    "            listener, print = setup_logging(log_filename, output_stream_to_console)\n",
    "            # local mode: Assume data is structured in output_path/split_type/ with CSV files\n",
    "            print(\n",
    "                f\"RUNNING LOCALLY GENERATED REPORTS: LABEL: run_from_localdir {run_from_localdir}\"\n",
    "            )\n",
    "            if all(files.values()):\n",
    "                print(f\"\\nProcessing dataset: {dataset_name} in {split_type}\")\n",
    "                # Extract the data for all three files\n",
    "                ground_truth_data = files[\"ground_truth\"]\n",
    "                report_types_data = files[\"report_types\"]\n",
    "                generated_reports_data = files[\"generated_reports\"]\n",
    "\n",
    "                print(ground_truth_data[\"file_name\"], ground_truth_data[\"report_type\"])\n",
    "                print(report_types_data[\"file_name\"], report_types_data[\"report_type\"])\n",
    "                print(\n",
    "                    generated_reports_data[\"file_name\"],\n",
    "                    generated_reports_data[\"report_type\"],\n",
    "                )\n",
    "\n",
    "                process_unit_time_module(\n",
    "                    dataset_name,\n",
    "                    ground_truth_data,\n",
    "                    report_types_data,\n",
    "                    generated_reports_data,\n",
    "                    split_type,\n",
    "                    log_filename,\n",
    "                    rg_model_name,\n",
    "                    verbose=is_verbose,\n",
    "                )\n",
    "\n",
    "            listener.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65089da2",
   "metadata": {},
   "source": [
    "# Shutdown SUTime client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74584d9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if sutime_client is not None:\n",
    "    sutime_client.stop()\n",
    "    sutime_client = None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tada_bench",
   "language": "python",
   "name": "tada_bench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
